\section{Evaluation}
\label{evaluation}

\subsection{Method}
\label{Method}
To evaluate our plug-in we will conduct a trial to highlight the benefits of using the code generation that the plug-in provides, as opposed to having developers write the code themselves. Specifically, we aim to show, as stated in the Goals section, that implementation time will be decreased, which encompasses both the coding and correcting of errors. We will base our test primarily on quantifiable data by measuring the difference in time spent creating a group of Intents to be inserted into in a pre-made Android application with just one Activity. The method stubs are left empty for the trial participants to fill out with the correct code, using either the plug-in or by manually writing the code.

The participants were seen as being successful in their trial when they had completed all the required code without errors for the given Intents. The Intents were chosen for their specific use - they were simple, but commonly used. One of the required Intents implementation also required that the developer modify the Android Manifest XML file with a permission required for the Intent to use - for the participants with the plug-in this is handled automatically, but those without had to ensure they correctly inserted it.

The participants largely made up of students at ITU but with different levels of programming experience, will be split into two groups based on Java programming experience. The least experienced will be instructed to use the plug-in, and the more experienced developers will be asked to do the implementation on their own but are allowed any Internet resource they require. The participants will be given a short introduction to the task first, where they will be given the opportunity to see the fully working application in action on an Android device. After this, the subjects will start the test and the timer started.

It was also considered having participants building the entire application from scratch, but we regressed from this idea because the many variables that this would introduce would make it difficult to dissect the results, and clearly establish the effects of using the plug-in.

\subsection{Results}
\label{Results}
We conducted the trial and recorded the results given in table below:

%Table: http://bit.ly/ZM6rqQ
\begin{center}
\begin{tabular}{|l|l|l|}
    \hline
    ~        & {\bf With the plug-in} & {\bf Without the plug-in} \\ \hline
    ~        & 1:20             & 2:59                \\ \hline
    ~        & 1:08             & 7:02                \\ \hline
    ~        & 1:30             & 4:20                \\ \hline
    ~        & 2:00             & 3:30                \\ \hline
    ~        & 1:09             & 3:20                   \\ \hline
    {\bf Average:} & 1:25             & 4:15                \\ \hline
\end{tabular}
\end{center}

As is palpably shown from the table, the average implementation time was cut severely by a factor of ~3, suggesting that the use of the plug-in may very well contribute to the effectiveness of the developer. These result were very much along the lines of what we had hoped for when we initially set out to create the prototype, and the vast gap in implementation time between the two groups clearly indicates that the use of code generation tools may be of use not only to cut the time spent typing and making documentation lookups, but also to further the correctness of what is written and reduce time spent bug-fixing as a project develops.

We decided to omit the focus on making quantitative measuring of the error rate during the trial. It would have been complicated to make a clear definition of what constitutes an error, for one. Should it only be something that prevents the code from compiling? Or is even a typing mistake to be considered an error, since even if the programmer corrects the mistake before compiling it has cost him a brief moment to correct? If the latter description was to be considered an error a much more thorough monitoring of the developer working during the trial would have to be carried out and even so it would still be hard to believe the measurements would be completely reliable.
With our set-up the participant is not done until the code is verified to be error free. From this we feel given the noticeable cut in implementation time, in spite of not having an accurate quantitative measure of it, that we can rightly infer our prototype has also had a positive impact on error reduction.  








